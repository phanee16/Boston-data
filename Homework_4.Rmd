---
title: "Homework_4"
author: "Phaneesha Chilaveni"
date: "11/7/2021"
output: html_document
---



#Load the all the required libraries for the assignment
```{r}
library(ISLR2)
library(ISLR)
library(bootstrap)
library(boot)
library(leaps)
library(klaR)
library(class)
library(GGally)
library(corrplot)
library(caret)
library("rpart")
```


#**Questions-1**#

##**1) For the Boston data in the ISLR2 package: > library(ISLR2) > data(Boston) > ?Boston Using best subset regression analysis fit models for “medv” (median value of owner-occupied homes in $1000s). Perform model selection using the AIC, BIC, five-and tenfold cross-validation, and bootstrap .632 estimates of prediction error. Comment on your results and the differences in the selected model.**###

#_Loading the Boston data from the ISLR2 package and copying it into a new variable boston for further manipulations on the data._
```{r}
data(Boston)
boston = Boston
dim(boston)
```


#_Splitting the data into training and testing by holding out 20% of the random boston data into testing and the remaining as training.Performing the Best subset regression analysis on the data._

```{r}

set.seed(500)
train = sample(1:nrow(boston),0.8*nrow(boston))
Y.train = boston$medv[train]
Y.test = boston$medv[-train]

training = boston[train,]
testing = boston[-train,]
```

#_Performing the model selection using AIC and BIC and finding out the best variable model._
```{r}
fit = regsubsets(training$medv~., data = training ,method = "exhaustive",nvmax = 13)
my_summary = summary(fit)
my_summary
names(my_summary)
my_summary$cp
my_summary$bic

which.min(my_summary$cp)

which.min(my_summary$bic)

```
##**Comment**: AIC predicted that 11 variable model is best where as BIC resulted that 10 varaiable model is best model


```{r}
#Just checking the function
train_errors = rep(NA,13)
test_errors = rep(NA,13)
train_pred_matrix = model.matrix(training$medv~.,data = training)

test_pred_matrix = model.matrix(testing$medv~.,data = testing)
for (i in 1:13) {
    coefi = coef(fit, id = i)
    pred_train <- train_pred_matrix[,names(coefi)] %*% coefi
    train_errors[i] = mean((Y.train - pred_train)^2)
    pred_test <- test_pred_matrix[,names(coefi)] %*% coefi
    test_errors[i] = mean((Y.test - pred_test)^2)
}
train_errors
min(train_errors)
which.min(train_errors)
test_errors
min(test_errors)
which.min(test_errors)
plot(train_errors, col = "blue", type = "b", xlab = "No. of variables", ylab = "Train MSE", pch = 16)
lines(test_errors,col = "red",type = "b")
```
#_Performing model selection using 5-fold cross validation_
```{r}
set.seed(120)
#Creating folds
fold <- createFolds(boston, k=5)


for (i in 1:length(fold)){
train_fold = boston[-fold[[i]],]
test_fold = boston[fold[[i]],]
Y.train = boston$medv[-fold[[i]]]
Y.test = boston$medv[fold[[i]]]
fit = regsubsets(train_fold$medv~., data = train_fold ,method = "exhaustive",nvmax = 13)
train_errors = rep(NA,13)
test_errors = rep(NA,13)
train_pred_matrix = model.matrix(train_fold$medv~.,data = train_fold)

test_pred_matrix = model.matrix(test_fold$medv~.,data = test_fold)
for (j in 1:13) {
    coefi = coef(fit, id = j)
    pred_train <- train_pred_matrix[,names(coefi)] %*% coefi
    train_errors[j] = mean((Y.train - pred_train)^2)
    pred_test <- test_pred_matrix[,names(coefi)] %*% coefi
    test_errors[j] = mean((Y.test - pred_test)^2)
}
#print(train_errors)
#print(min(train_errors))
#print(which.min(train_errors))
print(test_errors)
print(min(test_errors))
print(which.min(test_errors))
plot(train_errors, col = "blue", type = "b", xlab = "No. of variables", ylab = "Train MSE", pch = 16)
lines(test_errors,col = "red",type = "b")
}

```
##**Comment**:5-fold cross validation resulted that 2 variable model is best(as it came 4 times out of 5)



#_Performing model selection using 10-fold cross validation_
```{r}
set.seed(1)
#Creating folds
fold <- createFolds(boston, k=10)


for (i in 1:length(fold)){
train_fold = boston[-fold[[i]],]
test_fold = boston[fold[[i]],]
Y.train = boston$medv[-fold[[i]]]
Y.test = boston$medv[fold[[i]]]

fit = regsubsets(train_fold$medv~., data = train_fold ,method = "exhaustive",nvmax = 13)
train_errors = rep(NA,13)
test_errors = rep(NA,13)
train_pred_matrix = model.matrix(train_fold$medv~.,data = train_fold)

test_pred_matrix = model.matrix(test_fold$medv~.,data = test_fold)
for (j in 1:13) {
    coefi = coef(fit, id = j)
    pred_train <- train_pred_matrix[,names(coefi)] %*% coefi
    train_errors[j] = mean((Y.train - pred_train)^2)
    pred_test <- test_pred_matrix[,names(coefi)] %*% coefi
    test_errors[j] = mean((Y.test - pred_test)^2)
}
#print(train_errors)
#print(min(train_errors))
#print(which.min(train_errors))
print(test_errors)
print(min(test_errors))
print(which.min(test_errors))
plot(train_errors, col = "blue", type = "b", xlab = "No. of variables", ylab = "Train MSE", pch = 16)
lines(test_errors,col = "red",type = "b")
}

```

##**Comment**:10-fold cross validation resulted that 2 variable model is best 


#_Calculate bootstrap prediction error for the best models of size "k" by creating functions that fed into bootpred_
```{r}

#create functions that feed into a "bootpred"
beta.fit = function(X,Y){
  lsfit(X,Y)
}
beta.predict = function(fit,X){
  cbind(1,X)%*%fit$coef
  
}

sq.error = function(Y,Yhat){
  (Y-Yhat)^2
}

#create X and Y

X = boston[,1:13]
Y = boston[,14]
```


#_Search over the best possible subsets of size "k" for which the error is minimum_
```{r}
set.seed(123)
select = summary(fit)$outmat
error.store = c()
for (i in 1:13){
  temp = which(select[i,] == "*")
  res = bootpred(X[,temp],Y,nboot = 50,theta.fit = beta.fit,theta.predict = beta.predict,err.meas = sq.error)
  error.store = c(error.store,res[[3]])
}
error.store
plot(error.store)
which.min(error.store)
```
##**Comment**:Bootstrap .632 resulted that 11 variable model is best.So among all the results 11 variable model is found to be best for the data.


#**Question - 2**

##**2) Use the same Boston dataset that you used in Question 1. Fit classification models in order to predict whether a given census tract has a crime rate above or below the median. Explore logistic regression, LDA, knn and CART. Describe your findings. (Hint: you will have to “create” this new response variable from the “crim” variable)**###

#_To classify the crime rate above or below the median value we have to assign binary values for the crime rate(0 for crim rate below median and 1 for crime rate above median value)_
```{r}

boston_new = Boston
crime01 = rep(0, length(boston_new$crim))
crime01[boston_new$crim > median(boston_new$crim)] = 1
boston_new= data.frame(boston_new[,-1],crime01)
summary(boston_new)
```

#_Splitting the data into 50% as train and 50% as test for further manipulations on the data_
```{r}

set.seed(1)
train_new = sample(1:nrow(boston_new),0.5*nrow(boston_new))
train_data = boston_new[train_new,]
test_data = boston_new[-train_new,]
crime01_train = crime01[train_new]
crime01_test = crime01[-train_new]
```

#_Using corrplot to see any correlation between crime01 and other parameters_
```{r}
corrplot.mixed(cor(boston_new),upper = "circle")
```
##**Comment**:The crime01 is highly correlated to indus,nox,age,dis,rad and tax(taken only the variables which have correlation coefficient greater than 0.6)

#_Exploring the data using Logistic Regression (This is just to check the whole data)_
```{r}
crim.fit = glm(crime01~.-crime01 , data = train_data, family = binomial)
summary(crim.fit)
par(mfrow=c(2,2))
plot(crim.fit)
boston.probs = predict(crim.fit, test_data, type = "response",)
boston.pred = rep(0, length(boston.probs))
boston.pred[boston.probs > 0.5] = 1
table(boston.pred, crime01_test)
mean(boston.pred != crime01_test)
```
##**Comment**: We may conclude that, for this logistic regression considering the whole data, we have a test error rate of 9.881423%. 

#_Exploring the data using Logistic Regression by taking indus,nox,age,dis,rad and tax (which have correlation with crime01)_
```{r}
new_fit =  glm(crime01~indus+nox+rad+age+dis+tax, data = train_data, family = binomial)
summary(new_fit)
par(mfrow=c(2,2))
plot(new_fit)
boston.probs = predict(new_fit, test_data, type = "response",)
boston.pred = rep(0, length(boston.probs))
boston.pred[boston.probs > 0.5] = 1
table(boston.pred, crime01_test)
mean(boston.pred != crime01_test)
```
##**Comment**: We may conclude that, for this logistic regression considering the associated parameters, we have a test error rate of 11.46245%.

#_Exploring the data using Linear Discriminant Analysis for whole data with crime01_
```{r}
#LDA
lda.fit=lda(crime01~.-crime01, data=train_data)
lda.fit
plot(lda.fit)
lda.pred=predict(lda.fit,test_data)$class
table(lda.pred,test_data$crime01)
mean(lda.pred!=test_data$crime01)
```
##**Comment**: We may conclude that, for Linear Discriminant Analysis considering whole data, we have a test error rate of 16.20553%.


#_Exploring the data using Linear Discriminant Analysis for indus,nox,age,dis,rad and tax with crime01_
```{r}
#LDA
lda.fit=lda(crime01~indus+nox+rad+age+dis+tax, data=train_data)
lda.fit
plot(lda.fit)
lda.pred=predict(lda.fit,test_data)$class
table(lda.pred,test_data$crime01)
mean(lda.pred!=test_data$crime01)
```

##**Comment**: We may conclude that, for Linear Discriminant Analysis considering the associated variables , we have a test error rate of 15.81028%.

#_Exploring the data using KNN and finding the best k for the whole data _
```{r}
set.seed(1)
knn_pred = NULL
error_rate = NULL
for(i in 1:10){
knn_pred = knn(train_data,test_data,crime01_train,k=i)
error_rate[i] = mean(crime01_test != knn_pred)
pred=knn(train_data, test_data, crime01_test, k=i)
print(table(pred,crime01_test))
}
error_rate
plot(error_rate)
min(error_rate)
which.min(error_rate)

```
##**Comment** : We can conclude that the minimum error is 9.486166% for k=1 for the above KNN.

#_Exploring the data using KNN and finding the best k for the data which have indus,nox,rad,age,dis and tax_
```{r}
boston_train = cbind(train_data$indus+train_data$nox+train_data$rad+train_data$age+train_data$dis+train_data$tax)
boston_test = cbind(test_data$indus+test_data$nox+test_data$rad+test_data$age+test_data$dis+test_data$tax)
knn_pred = NULL
error_rate = NULL
for(i in 1:10){
set.seed(1)
knn_pred = knn(boston_train,boston_test,crime01_train,k=i)
error_rate[i] = mean(crime01_test != knn_pred)
pred=knn(boston_train, boston_test, crime01_test, k=i)
print(table(pred,crime01_test))
}
error_rate
plot(error_rate)
min(error_rate)
which.min(error_rate)

```
##**Comment** : We can conclude that the minimum error is 14.62451% for k=3 for the above KNN.

#_Classification and Regression Trees - Full Tree (This is done for whole data)_
```{r}
#Classification -full tree and pruned tree
set.seed(1)
model.control <- rpart.control(minsplit = 5, xval = 13, cp = 0)
fit <- rpart(crime01~., data = train_data, method = "class", control = model.control)

plot(fit$cptable[,4], main = "Cp for model selection", ylab = "Cp")

min_cp = which.min(fit$cptable[,4])
pruned_fit <- prune(fit, cp = fit$cptable[min_cp,1])

## plot the full tree and the pruned tree

plot(pruned_fit,uniform = TRUE, branch = .3, compress=T, main = "Pruned Tree")
text(pruned_fit,use.n=TRUE, all=TRUE, cex = .5)


plot(fit,uniform = TRUE, branch = .3, compress=T, main = "Full Tree")
text(fit,use.n=TRUE, all=TRUE, cex = .5)
```
#Regression -Full Tree

```{r}
set.seed(1)
model.controls <- rpart.control(minbucket = 1, minsplit = 2, xval = 13, cp = 0)

fit_boston <- rpart(crime01~., data = train_data, control = model.controls)

plot(fit_boston$cptable[,4], main = "Cp for model selection", ylab = "cv error")

plot(fit_boston, uniform = TRUE,branch = .3, compress=T, main = "Full Tree")
text(fit_boston,use.n=TRUE, all=TRUE, cex = .5)
plotcp(fit_boston)
rsq.rpart(fit_boston)#to visualize cross-validation results


pred = predict(fit_boston,newdata = test_data)
unique(pred)
mean(pred!=crime01_test)
table(pred,crime01_test)
```
#**Comment**: The test error rate from the CART is 6.719368% for the Full Tree.

#_Classification and Regression Trees - Pruned Tree_
```{r}
set.seed(1)
min_cp = which.min(fit_boston$cptable[,4])#4th column is the error
min_cp
pruned_fit_boston <- prune(fit_boston, cp = fit_boston$cptable[min_cp, 1])

plot(pruned_fit_boston, uniform = TRUE,branch = .3, compress=T, main = "Pruned Tree")
text(pruned_fit_boston,use.n=TRUE, all=TRUE,cex = .5)
plotcp(pruned_fit_boston)
rsq.rpart(pruned_fit_boston)#to visualize cross-validation results

pred_prune <- predict(pruned_fit_boston, newdata = test_data)
unique(pred_prune)
pred_prune[pred_prune<0.5]=0
mean(pred_prune!=crime01_test)
table(pred_prune,crime01_test)

```
#**Comment**: The test error rate from the CART is 5.928854% for the Pruned Tree.

#_Classification and Regression - for the variables that are associated with crime01_
```{r}
#Classification -full tree and pruned tree
set.seed(1)
model.control <- rpart.control(minsplit = 5, xval = 13, cp = 0)
fit <- rpart(crime01~indus+nox+rad+age+dis+tax, data = train_data, method = "class", control = model.control)


plot(fit$cptable[,4], main = "Cp for model selection", ylab = "Cp")

min_cp = which.min(fit$cptable[,4])
pruned_fit <- prune(fit, cp = fit$cptable[min_cp,1])

## plot the full tree and the pruned tree

plot(pruned_fit, uniform = TRUE,branch = .3, compress=T, main = "Pruned Tree")
text(pruned_fit, use.n=TRUE, all=TRUE,cex = .5)


plot(fit,uniform = TRUE, branch = .3, compress=T, main = "Full Tree")
text(fit, use.n=TRUE, all=TRUE,cex = .5)
```

#_Regression -Full Tree_
```{r}

set.seed(1)
model.controls <- rpart.control(minbucket = 1, minsplit = 2, xval = 13, cp = 0)

fit_boston <- rpart(crime01~indus+nox+rad+age+dis+tax, data = train_data, control = model.controls)

plot(fit_boston$cptable[,4], main = "Cp for model selection", ylab = "cv error")

plot(fit_boston, uniform = TRUE,branch = .3, compress=T, main = "Full Tree")
text(fit_boston,use.n=TRUE, all=TRUE, cex = .5)
plotcp(fit_boston)
rsq.rpart(fit_boston)#to visualize cross-validation results


pred = predict(fit_boston,newdata = test_data)
unique(pred)
mean(pred!=crime01_test)
table(pred,crime01_test)
```
#**Comment**: The test error rate from the CART is 10.67194% for the Full Tree._

#_Regression - Pruned Tree_
```{r}
set.seed(1)
min_cp = which.min(fit_boston$cptable[,4])#4th column is the error
min_cp
pruned_fit_boston <- prune(fit_boston, cp = fit_boston$cptable[min_cp, 1])

plot(pruned_fit_boston, uniform = TRUE,branch = .3, compress=T, main = "Pruned Tree")
text(pruned_fit_boston,use.n=TRUE, all=TRUE,cex = .5)
plotcp(pruned_fit_boston)
rsq.rpart(pruned_fit_boston)#to visualize cross-validation results

pred_prune <- predict(pruned_fit_boston, newdata = test_data)
unique(pred_prune)
pred_prune[pred_prune<0.5]=0
mean(pred_prune!=crime01_test)
table(pred_prune,crime01_test)

```
#**Comment**: The test error rate from the CART is 11.46245% for the Pruned Tree._


#**Question - 3**

##**In this problem, you will develop a model to predict whether given car gets high or low gas mileage based on the Auto data set.**##

#_Load the data_
```{r}
data(Auto)
auto = Auto
summary(auto)
str(auto)
```

##** (a) Create a binary variable, mpg01, that contains a 1 if mpg contains a value above its median, and a 0 if mpg contains a value below its median. You can compute the median using the median() function. Note you may find it helpful to use the data.frame() function to create a single data set containing both mpg01 and the other Auto variables.**##

#_Creating the binary variable mpg01 and adding it to the original data_
```{r}
mpg01 = rep(0, length(auto$mpg))
mpg01[auto$mpg > median(auto$mpg)] = 1
auto = data.frame(auto[,-1], mpg01)
```


##**(b) Explore the data graphically in order to investigate the association between mpg01 and the other features. Which of the other features seem most likely to be useful in predicting mpg01? Scatterplots and boxplots may be useful tools to answer this question. Describe your findings.**##

#_Using corrplot to see any correlation between mpg01 and other parameters_
```{r}
corrplot.mixed(cor(auto[, -8]), upper="circle")
```

##**Comment** : From the corrplot mpg01 has high correlation with cylinders,displacement,horsepower and weight 

#_Using Scatterplot to see any relationship between the variables_

```{r}
cols <- character(nrow(auto))
cols[auto$mpg01 == 0]<-"purple2"
cols[auto$mpg01 == 1] <- "olivedrab2"

#Scatterplot
pairs(~mpg01+cylinders+displacement+horsepower+weight+acceleration+year,data = auto,col=cols,pch = 16,cex = 0.75)
```
#**Comment**: There is a postive relationship between displacement,horsepower and weight among each other and they have a negative relationship with acceleration

#_Boxplots_
```{r}
par(mfrow=c(2,3))
boxplot(cylinders ~ mpg01, data = auto, main = "Cylinders vs mpg01",col="green")
boxplot(displacement ~ mpg01, data = auto, main = "Displacement vs mpg01",col = "red")
boxplot(horsepower ~ mpg01, data = auto, main = "Horsepower vs mpg01",col = "magenta")
boxplot(weight ~ mpg01, data = auto, main = "Weight vs mpg01",col = "paleturquoise1")
boxplot(acceleration ~ mpg01, data = auto, main = "Acceleration vs mpg01",col = "orange")
boxplot(year ~ mpg01, data = auto, main = "Year vs mpg01",col = "lightslateblue")


```


##**(c) Split the data into a training set and a test set**##

#_Splitting the data into half training and half testing_

```{r}
set.seed(1)
index = sample(1:nrow(auto),nrow(auto)*0.5)
auto_train = auto[index,]
auto_test = auto[-index,]
mpg01_train = mpg01[index]
mpg01_test = mpg01[-index]

```


##**(d) Perform LDA on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in (b). What is the test error of the model obtained?**##

#_Performing Linear Discriminant Analysis and getting the test error for the variables that are most associated with mpg01_
```{r}
#LDA
lda.fit=lda(mpg01~cylinders+displacement+horsepower+weight+acceleration+year, data=auto_train)
lda.fit
plot(lda.fit)
lda.pred=predict(lda.fit,auto_test)$class
table(lda.pred,auto_test$mpg01)
mean(lda.pred!=auto_test$mpg01)
```
##**Comment**: We may conclude that, for Linear Discriminant Analysis, we have a test error rate of 12.7551%.

##**(e) Perform QDA on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in (b). What is the test error of the model obtained?**##

#_Performing Quadratic Discriminant Analysis and getting the test error for the variables that are most associated with mpg01_
```{r}
#QDA
qda.fit=qda(mpg01~cylinders+displacement+horsepower+weight+acceleration+year, data=auto_train)
qda.fit
qda.pred=predict(qda.fit,auto_test)$class
table(qda.pred,auto_test$mpg01)
mean(qda.pred!=auto_test$mpg01)
```
##**Comment**: We may conclude that, for Quadratic Discriminant Analysis, we have a test error rate of 9.693878%.

##**(f) Perform logistic regression on the training data to predict mpg01 using the variables that seemed most associated with mpg01 in (b). What is the test error of the model obtained?**##

#_Performing Logistic Regression and getting the test error for the variables that are most associated with mpg01_
```{r}
glm.fit=glm(mpg01~cylinders+displacement+horsepower+weight+acceleration+year, data=auto_train,family = binomial)
glm.fit
par(mfrow = c(2,2))
plot(glm.fit)
auto.probs = predict(glm.fit, auto_test, type = "response",)
auto.pred = rep(0, length(auto.probs))
auto.pred[auto.probs > 0.5] = 1
table(auto.pred, mpg01_test)
mean(auto.pred != mpg01_test)
```
##**Comment**: We may conclude that, for Logistic Regression, we have a test error rate of 8.673469%.

##**(h) Perform KNN on the training data, with several values of K, in order to predict mpg01. Use only the variables that seemed most associated with mpg01 in (b). What test errors do you obtain? Which value of K seems to perform the best on this data set?**##

#_Performing KNN with K values from 1 to 10 and getting the test error for the variables that are most associated with mpg01_
```{r}
auto_training = cbind(auto_train$cylinders+auto_train$displacement+auto_train$horsepower+auto_train$weight+auto_train$acceleration+auto_train$year)
auto_testing = cbind(auto_test$cylinders+auto_test$displacement+auto_test$horsepower+auto_test$weight+auto_test$acceleration+auto_test$year)
knn_pred = NULL
error_rate = NULL
for(i in 1:10){
set.seed(1)
knn_pred = knn(auto_training,auto_testing,mpg01_train,k=i)
error_rate[i] = mean(mpg01_test != knn_pred)
pred=knn(auto_training, auto_testing, mpg01_test, k=i)
print(table(pred,mpg01_test))
}
error_rate
plot(error_rate)
min(error_rate)
which.min(error_rate)

```

##**Comment**: The error rate is minimum for 8 variables with error rate as 11.22449%. After reviewing the results of each classification method, Logistic Regression has the least error of 8.673469% followed by Quadratic Discriminant Analysis. 

















